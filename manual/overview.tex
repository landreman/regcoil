\chapter{Overview}

This program computes efficiency-ordered distributions of magnetic fields as described in \cite{boozer2015}.  The main method is described on page 12, and some key definitions are given on page 9.

There are some similarities between this program and the \nescoil~code used to compute stellarator coils \cite{nescoil}.
\nescoil~effectively computes the `inductance' between a plasma surface and a coil surface, meaning the linear
relationship by which the current potential on the coil surface gives rise to a normal component of the magnetic field $\vect{B}$ on the plasma
surface.  \nescoil~takes the normal component of $\vect{B}$ on the plasma surface associated with net poloidal coil current plus plasma
current, and effectively multiplies the result by the (pseudo)inverse of the inductance matrix, yielding the current potential required
to achieve a magnetic surface (zero normal component of $\vect{B}$) on the plasma surface.  In \bdistrib, however,
we do not consider net coil current or plasma current, and we compute two inductance matrices instead of one.  The two inductance matrices
in \bdistrib~refer to the same outer surface (on which the current potential is defined), but to different surfaces on which the normal $\vect{B}$
is evaluated. Multiplying one inductance matrix by the (pseudo)inverse of the other gives the so-called `transfer matrix'.
Applying a singular value decomposition (SVD) to this transfer matrix, the singular vectors represent modes of the magnetic field,
and the singular values represent the efficiency by which these modes propagate from one toroidal surface to the other.

In \bdistrib~the three toroidal surfaces are named `plasma', `middle', and `outer', in order from the innermost to outermost. The
`plasma' surface can correspond to the outermost surface of a \vmec~equilibrium, or it can be a plain circular toroidal surface. The name
of the relevant surface appears as a suffix on most variables.

On each surface we use a poloidal angle $u$ and a toroidal angle $v$,
defined as in \nescoil.  The coordinate $u$ lies in the range $[0,1]$. The
surfaces have \nfp~identical toroidal periods, and an increase in $v$
by 1 corresponds to 1 of these periods. Thus, $v$ increases by \nfp~in a
complete toroidal revolution. In the output file, there is an array {\ttfamily v}
corresponding to one toroidal period, as well as an array {\ttfamily vl}
corresponding to all \nfp~toroidal periods.  Note that $v$ is
proportional to the standard cylindrical angle $\phi$: $\phi = 2\pi v/$\nfp.
Generally, field lines are not straight in the $(u,v)$ coordinates.  On
the plasma surface, $u$ is identical to the \vmec~poloidal angle divided
by $2\pi$, while the \vmec~toroidal angle differs by $2\pi/$\nfp compared to $v$.

In the various variable names in the code and output file, `r' refers
to the position vector, not to a radius.  In various arrays with a
dimension of length 3, this dimension always corresponds to Cartesian
coordinates $(x,y,z)$.

The `normal' quantities in the code and output file refer to the
surface normal vector N = (dr/dv) cross (dr/du) as in the NESCOIL
paper. Note that this vector does not have unit magnitude.



\section{Required libraries}

\begin{itemize}

\item {\ttfamily LIBSTELL} (for reading \vmec~{\ttfamily wout} files, and for the {\ttfamily ezcdf} subroutines)
\item {\ttfamily NetCDF} (for writing the output file)
\item {\ttfamily BLAS/LAPACK} (for matrix multiplication and the SVD subroutine)

\end{itemize}

If {\ttfamily OpenMP} is available, calculations with the code are parallelized.  The plotting and testing functions use \python,
{\ttfamily numpy}, and {\ttfamily scipy}.
The plotting routines \bdistribPlot~and {\ttfamily compareSingularValuesPlot} use {\ttfamily matplotlib}.

\section{Parallelization}

The code does not use {\ttfamily MPI}, and so it runs on a single computing node.  However, it is possible to use multiple threads
on the node to accelerate computations.  The multi-threaded parallelization is done in part using {\ttfamily OpenMP}
and in part using a multi-threaded {\ttfamily BLAS} routine.

The slowest steps in \bdistrib~occur when assembling the two
inductance matrices.  For each of these two matrices, there are two slow steps.
The first is computation of the magnetic dipole formula between each pair of points
on the two toroidal surfaces.  The loop for this computation is parallelized using {\ttfamily OpenMP}.
The other slow step is the integration of this result against each of the basis functions,
which is done using matrix multiplication with the {\ttfamily BLAS} subroutine {\ttfamily DGEMM}.
To parallelize this step you can link \bdistrib~with a multi-threaded  {\ttfamily BLAS} library,
such as the Intel Math Kernel Library (MKL).

\section{\ttfamily make test}

To test that your \bdistrib~executable is working, you can run {\ttfamily make test}.  Doing so will run
\bdistrib~for some or all of the examples in the {\ttfamily examples/} directories.
After each example completes, several of the output quantities (typically the singular values
of the transfer matrix)
will be checked, using the
{\ttfamily tests.py} script in the example's directory.
The {\ttfamily make test} feature is very useful when making changes to the code, since it allows you to check
that your code modifications have not broken anything and that previous results
can be recovered.

If you run {\ttfamily make retest},
no new runs of \bdistrib~will be performed, but the {\ttfamily tests.py} script
will be run on any existing output files in the \path{/examples/} directories.


\section{Questions, Bugs, and Feedback}

We welcome any contributions to the code or documentation.
For write permission to the repository, or to report any bugs, provide feedback, or ask questions, contact Matt Landreman at
\href{mailto:matt.landreman@gmail.com}{\nolinkurl{matt.landreman@gmail.com} }






